---
title: "573 Group Project"
output: pdf_document
date: "`r Sys.Date()`"
---

# The Model

## Preparing the Data

```{r}
data <- read.csv("train.csv")
data[is.na(data)]<-0

# factor the appropriate columns
data$MSSubClass <- factor(data$MSSubClass)
data$MSZoning <- factor(data$MSZoning)
data$Street <- factor(data$Street)
data$Alley <- factor(data$Alley)
data$LotShape <- factor(data$LotShape)
data$LandContour <- factor(data$LandContour)
data$Utilities <- factor(data$Utilities)
data$LotConfig <- factor(data$LotConfig)
data$LandSlope <- factor(data$LandSlope)
data$Neighborhood <- factor(data$Neighborhood)
data$Condition1 <- factor(data$Condition1)
data$Condition2 <- factor(data$Condition2)
data$BldgType <- factor(data$BldgType)
data$HouseStyle <- factor(data$HouseStyle)
data$RoofStyle <- factor(data$RoofStyle)
data$RoofMatl <- factor(data$RoofMatl)
data$Exterior1st <- factor(data$Exterior1st)
data$Exterior2nd <- factor(data$Exterior2nd)
data$MasVnrType <- factor(data$MasVnrType)
data$ExterQual <- factor(data$ExterQual)
data$ExterCond <- factor(data$ExterCond)
data$Foundation <- factor(data$Foundation)
data$BsmtQual <- factor(data$BsmtQual)
data$BsmtCond <- factor(data$BsmtCond)
data$BsmtExposure <- factor(data$BsmtExposure)
data$BsmtFinType1 <- factor(data$BsmtFinType1)
data$BsmtFinType2 <- factor(data$BsmtFinType2)
data$Heating <- factor(data$Heating)
data$HeatingQC <- factor(data$HeatingQC)
data$CentralAir <- factor(data$CentralAir)
data$Electrical <- factor(data$Electrical)
data$KitchenQual <- factor(data$KitchenQual)
data$Functional <- factor(data$Functional)
data$FireplaceQu <- factor(data$FireplaceQu)
data$GarageType <- factor(data$GarageType)
data$GarageFinish <- factor(data$GarageFinish)
data$GarageQual <- factor(data$GarageQual)
data$GarageCond <- factor(data$GarageCond)
data$PavedDrive <- factor(data$PavedDrive)
data$PoolQC <- factor(data$PoolQC)
data$Fence <- factor(data$Fence)
data$MiscFeature <- factor(data$MiscFeature)
data$SaleType <- factor(data$SaleType)
data$SaleCondition <- factor(data$SaleCondition)
```

## Exploratory visuals

This confirms that the data broadly looks how we expect it to.

```{r}
plot(data$OverallQual,log(data$SalePrice))

bp <- boxplot(log(data$SalePrice) ~ data$Exterior1st, data = data, col = c("lightblue", "darkblue"), xaxt = "n")
tick <- seq_along(bp$names)
axis(1, at = tick, labels = FALSE)
text(tick, par("usr")[3] - 0.3, bp$names, srt = 90, xpd = TRUE)
plot(data$FullBath,log(data$SalePrice))
```

Here, I examine which factor variables might be worth dropping.

```{r}
library(ggplot2)

plotbar <- function(i){
     if (is.factor(get(i))){
         ggplot(data, aes(x=get(i))) + geom_bar() + ggtitle(i)
     }
}
```

Breaking this into two chunks to more easily isolate the function

```{r}
attach(data)
library(gridExtra)

plots <- lapply(names(data),plotbar)
plots[sapply(plots, is.null)] <- NULL # drop the null values
length(plots)
gridExtra::grid.arrange( grobs = plots, nrow = 7) # you might want to run this row separately and use the 'zoom' feature to the right to pop out the result for better visibility
```

```{r}
# Clearing data - Lily
library(forcats)

data.cl <- data[!sapply(data,is.factor)][-1]

#SaleCondition
plotbar("SaleCondition") #to find how to aggregate
data.cl$SaleCondition <- fct_collapse(SaleCondition,Abnormal= c("Abnorml","AdjLand","Alloca","Family"),Normal="Normal", Partial="Partial")

#SaleType
plotbar("SaleType") #to find how to aggregate
data.cl$SaleType <- fct_collapse(SaleType,New = "New",Warranty = c("WD","CWD", "VWD"), Others = c ("COD", "Con", "ConLw", "ConLl", "ConLD", "Oth"))

#Fence
plotbar("Fence") #to find how to aggregate
data.cl$Fence <- fct_collapse(Fence,No ="0",Good = c("GdPrv","GdWo"),Mini = c("MnPrv","MnWw"))

#PavedDrive
plotbar("PavedDrive") #to find how to aggregate
data.cl$PavedDrive <- fct_collapse(PavedDrive,NP= c("N","P"))

#GarageType
plotbar("GarageType")
data.cl$GarageType <- fct_collapse(GarageType,Attached = c("Attchd", "BuiltIn"),Detached = "Detchd",Other = c("CarPort","Basment","2Types"),None = "0")


#FireplaceQu
plotbar("FireplaceQu")
data.cl$FireplaceQu <- fct_collapse(FireplaceQu,Good=c("Ex","Gd"), Aver = c("TA","Fa"), Bad=c("Po","0"))

#Electrical
plotbar("Electrical")
data.cl$Electrical <- fct_collapse(Electrical,Stand = "SBrkr", Other = c("FuseA","FuseF","FuseP", "Mix",0))

#HeatingQC
plotbar("HeatingQC")
data.cl$HeatingQC <- fct_collapse(HeatingQC,Good =  c("Ex","Gd"),	Ave = "TA",	Bad = c("Fa","Po"))

#"BsmtExposure"
plotbar("BsmtExposure")
data.cl$BsmtExposure <- fct_collapse(BsmtExposure, Good =  "Gd",AbovMin = c("Av","Mn"), NoE = "No", NoB = "0")

#"BsmtCond"
plotbar("BsmtCond")
data.cl$BsmtCond <- fct_collapse(BsmtCond, Typical = "TA",NonTy = c("Ex","Gd","Fa","Po","0"))

#"BsmtQual"
plotbar("BsmtQual")
data.cl$BsmtQual <- fct_collapse(BsmtQual,Good=c("Ex","Gd"),	Aver = c("TA","Fa"),	Bad=c("Po","0"))


#"Foundation"
plotbar("Foundation")
data.cl$Foundation <- fct_collapse(Foundation,BrkTil= "BrkTil",CBlock = "CBlock",PConc = "PConc",Oth = c("Slab","Stone","Wood"))

#"ExterCond"
plotbar("ExterCond")
data.cl$ExterCond <- fct_collapse(ExterCond,Good =  c("Ex","Gd"),	Ave = "TA",	Bad = c("Fa","Po"))

#ExterQual
plotbar("ExterQual")
data.cl$ExterQual <- fct_collapse(ExterQual,Good =  c("Ex","Gd"), Ave = "TA",	Bad = c("Fa","Po"))

#"MasVnrType"
plotbar("MasVnrType")
data.cl$MasVnrType <- fct_collapse(MasVnrType, Brk= c("BrkCmn","BrkFace")	,Stone = "Stone",	Other = c("CBlock", "None"))

#"Exterior1st"
plotbar("Exterior1st")
data.cl$Exterior1st <- fct_collapse(Exterior1st, Brk = c("BrkComm","BrkFace"), Wood = c("Wd Sdng", "WdShing","HdBoard","Plywood"), Vinyl = "VinylSd", Metal = "MetalSd", Stone_Cement = c("Stone","CemntBd", "CBlock","PreCast"), Other = c("AsbShng","AsphShn","ImStucc","Stucco","Other") )

#"Exterior2nd"
plotbar("Exterior2nd")
data.cl$Exterior2nd <- fct_collapse(Exterior2nd, Brk = c("BrkComm","BrkFace"), Wood = c("Wd Sdng", "WdShing","HdBoard","Plywood"), Vinyl = "VinylSd", Metal = "MetalSd", Stone_Cement = c("Stone","CemntBd", "CBlock","PreCast"), Other = c("AsbShng","AsphShn","ImStucc","Stucco","Other") )

#"RoofStyle"
plotbar("RoofStyle")
data.cl$RoofStyle <- fct_collapse(RoofStyle,Gable = "Gable",	Hip = "Hip",	Others = c ("Flat","Gambrel","Mansard","Shed"))

#"HouseStyle"
plotbar("HouseStyle")
data.cl$HouseStyle <- fct_collapse(HouseStyle,OneStory = "1Story", OnenHalfStory = c ("1.5Fin", "1.5Unf")	, TwoStory = "2Story"	, TwonHalfStory = c ("2.5 Fin", "2.5Unf"),	Split = c("SFoyer", "SLvl"))


#"BldgType"
plotbar("BldgType")
data.cl$BldgType <- fct_collapse(BldgType,OneFam = "1Fam",	TwoFam = c ( "2FmCon", "Duplex") 	, Twn = c ( "TwnhsE", "Twnhsl"))

#"Condition1"
plotbar("Condition1")
data.cl$Condition1 <- fct_collapse(Condition1, Norm = "Norm", Other = c ("Artery", "Feedr", "RRNn", "RRAn", "PosN","PosA", "RRNe", "RRAe"))

#"Neighborhood"
plotbar("Neighborhood")
data.cl$Neighborhood <- fct_collapse(Neighborhood, North = c ("NWAmes", "NAmes", "NoRidge", "NPkVill", "NridgHt", "Somerst"),	West = c ("Sawyer", "SawyerW", "Gilbert", "SWISU", "Timber"),	East = c ("Edwards", "BrkSide", "Blueste", "BrDale", "IDOTRR", "MeadowV"),	Central = c("OldTown", "CollgCr", "Crawfor", "Mitchel"),	HighEnd = c ("Veenker", "StoneBr", "ClearCr"), Other = c ("Blmngtn"))

#"LotConfig"
plotbar("LotConfig")
data.cl$LotConfig <- fct_collapse(LotConfig,Standard = c ("Inside", "Corner"),	Premium = c ("CulDSac", "FR2", "FR3"))

#"LotShape"
plotbar("LotShape")
data.cl$LotShape <- fct_collapse(LotShape,Regular = "Reg", Irregular = c("IR1","IR2","IR3"))

#"MSZoning"
plotbar("MSZoning")
data.cl$MSZoning <- fct_collapse(MSZoning,Residentiallow = c ("RL", "RP")	,ResidentialMedHi = c ("RM", "RH") ,	Other = c( "FV", "A", "I", "C")	)

#"MSSubClass"
plotbar("MSSubClass")
data.cl$MSSubClass <- fct_collapse(MSSubClass, story1=c(20,30,40,120), story1.5=c(45,50,150),story2=c(60,70,160),other=c(75,80,85,90,180,190))
```

## linear regression - Ari

Since the outcome variable is housing prices, we will examine whether log transformation of the outcome variable will better fit our data:

```{r}
linbase <- glm(SalePrice~.,data = data.cl)
linlog <- glm(log(SalePrice)~., data = data.cl)
summary(linbase)
summary(linlog)
```

```{r}
library(boot)
cv.glm(data.cl, linbase , K = 5)$delta[1]
cv.glm(data.cl, linlog , K = 5)$delta[1]
```

Keep in mind that, since y-values are on different axes, it is entirely inappropriate to compare cv output. This was a proof of concept. See the residual plots below for model evaluations:

```{r}
par(mfrow=c(2,2))
plot(linbase)
plot(linlog)
```

By visual inspection, the models seem similar, though the log outcome variable may be slightly more appropriate, which aligns with our intuition that prices should be measured on a log scale.

## LASSO/ridge/elastic net - Ari

```{r}
library(glmnet)

x <- model.matrix(log(SalePrice)~.,data.cl)[, -1]
y <- log(data.cl$SalePrice)
```

### Lasso

```{r}
grid <- 10^ seq(10, -2, length = 100)
lasso.mod <- glmnet(x, y, alpha = 1, lambda = grid)
plot(lasso.mod, main="Lasso")
```

Cross Validation:

```{r}
cv.out <- cv.glmnet(x,y, alpha = 1)
plot(cv.out)
cv.out$lambda.min
min(cv.out$cvm)
```

Using a lasso regression, the lowest mean Cross-Validated error is 0.02154846, with lambda of 0.003751818

### Ridge

```{r}
grid <- 10^ seq(10, -2, length = 100)
ridge.mod <- glmnet(x, y, alpha = 0, lambda = grid)
plot(ridge.mod, main="ridge")
```

Cross Validation:

```{r}
cv.out <- cv.glmnet(x,y, alpha = 0)
plot(cv.out)
cv.out$lambda.min
min(cv.out$cvm)
```

Using a ridge regression, the lowest mean Cross-Validated error is 0.02218426, with lambda of 0.0625841

### Elastic Net

```{r}
grid <- 10^ seq(10, -2, length = 100)
ridge.mod <- glmnet(x, y, alpha = .5, lambda = grid)
plot(ridge.mod, main="Elastic Net")
```

Cross Validation:

```{r}
cv.out <- cv.glmnet(x,y, alpha = .5)
plot(cv.out)
cv.out$lambda.min
min(cv.out$cvm)
```

Using an elastic net with alpha of 0.5, the lowest mean Cross-Validated error is 0.02203126, with lambda of 0.005171966.

## PCR and PLS - Ari

Beginning with PCR:

```{r}
library(pls)
```

```{r}
pcr.fit <- pcr(log(SalePrice) ~., data = data.cl, scale = TRUE, validation = "CV")
summary(pcr.fit)
validationplot(pcr.fit, val.type = "MSEP")
```

We see that the CV levels out at around 9 components, with few, if any, improvements to CV beyond that point. The CV-derived MSE associated with 9 components is $0.1589^2 = 0.02524921$, which is higher than the lasso, ridge, and elastic net regressions.

Since we have an outcome variable, supervised learning is possible. Consequently, Partial Least Squares is preferred to Principal Component Analysis.

```{r}
pls.fit <- plsr(log(SalePrice) ~., data = data.cl, scale = TRUE, validation = "CV")
summary(pls.fit)
validationplot(pls.fit, val.type = "MSEP")
```

We see that the CV levels out at around 3 components, with few, if any, improvements to CV beyond that point. The CV-derived MSE associated with 3 components is $0.1520^2 = 0.023104$, which is higher than the lasso, ridge, and elastic net regressions, but lower than PCR.

## regression trees and random forests

```{r}
library(tree)
```

### The tree

```{r}
tree.reg <- tree(log(SalePrice)~., data = data.cl, control = tree.control(nobs = length(data.cl[,1]), mindev = 0) )
summary(tree.reg)
plot(tree.reg)
text(tree.reg, pretty = 0)
cv.reg <- cv.tree(tree.reg, K=10)
print(cv.reg)
```

```{r}
plot(cv.reg$size, cv.reg$dev, type = "b")
```

The optimal tree size appears to be 11. Let's make one of th that size

```{r}
pruned <- prune.tree(tree.reg, best = 11)
plot(pruned)
text(pruned, pretty = 0)
```

Referring back to the cross-validation, we see that thesum of squared errors for the size-eleven tree is 77.12415. To get the *Mean Squared Error* we divide by the number of observations: $\frac{77.12415}{1460} =  0.05282476$. This is considerably worse than dimension reduction or regularization approaches, as would be expected.

### Random Forest

```{r}
library(randomForest)
rf <- randomForest(log(SalePrice)~., data = data.cl, importance = TRUE)# use default mtry values
varImpPlot(rf)
mean(rf$mse)
```

MSE [appears to be](https://datajobs.com/data-science-repo/Random-Forest-%5bLiaw-and-Weiner%5d.pdf) based on out of bag predictions, so I use that in lieu of K-fold cross validation to get an MSE of 0.02006823, which is competitive with — if not better than — dimension reduction or regularization approaches.

## Boosting

```{r}
library(gbm)
boost <- gbm(log(SalePrice)~., data = data.cl, distribution = "gaussian", n.trees = 5000, interaction.depth = 4, cv.folds = 5)
summary(boost)
plot(boost, i = "OverallQual")
plot(boost, i = "GrLivArea")
mean(boost$cv.error)
```
With 5-fold cross-validation, the MSE was 0.01866449. This is a dramatic improvement from the single tree.

## Support Vector Machines

```{r}
library(e1071)
tune.svm.rad <- tune(svm, log(SalePrice)~., data = data.cl, kernel = "radial", ranges = list(cost = c(0.0001, 0.001, 0.01, 0.1, 1, 5, 10)))
summary(tune.svm.rad)
svm.radial <- svm(log(SalePrice)~., data = data.cl, kernel = "radial", scale = TRUE, cost = 1, gamma = 1)
summary(svm.radial)
tune.svm.lin <- tune(svm, log(SalePrice)~., data = data.cl, kernel = "linear", ranges = list(cost = c(0.0001, 0.001, 0.01, 0.1, 1, 5, 10)))
summary(tune.svm.lin)
svm.linear <- svm(log(SalePrice)~., data = data.cl, kernel = "linear", cost = 5)
summary(svm.linear)
```

10-fold cross validation indicated that using a cost of 1 was ideal for a Support Vector Regression with a radial kernel, having an MSE of 0.01731201. By contrast, a cost of 5 was ideal for a SVR with a linear kernel, corresponding with an MSE of 0.02176070.

## kNN

```{r}
library(class)
library(caret)
```

Use k-fold KNN to identify the best model:

```{r, echo=FALSE}
fullfactors <- append(sapply(data.cl,is.factor),rep(FALSE,158-62))
dummydata <- dummy_columns(data.cl)[!fullfactors]

mod_knn = train(x = dummydata,y = log(dummydata$SalePrice),method = "knn", preProcess = c("center", "scale"), tuneGrid = data.frame(k = c(1:15)), trControl = trainControl(method = "repeatedcv", number = 10, repeats = 3))

mod_knn
```

The optimal value for K was 10 (when testing a range from 1 to 15), with a 10-fold CV RMSE of 0.1832944, notably much higher than other types of models.
